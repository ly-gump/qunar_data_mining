---
documentclass: ctexart
title: "R_process"
output: rticles::ctex
---
---
output: pdf_document
geometry: "left=3cm,right=3cm,top=3cm,bottom=2cm"
---
\pagestyle{plain}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=8, fig.height=6, warning=FALSE, message=FALSE)
```


首先导入数据，进行简单的预处理，由于响应变量极差很大，我们对响应变量进行对数变换，并且标准化；而后将非分类型变量数据标准化。
```{r}
# 导入数据
rm(list=ls())
set.seed(123)
data = read.csv('regression_data2.csv',encoding = "UTF-8")
y = data['read_num_transformed'] 
y = log(y) # 对响应变量进行对数变换
data['read_num_transformed'] = y
X_num = data[c("per_cost","travel_days","photo_num","title_length")]
X_num = scale(X_num,scale=TRUE)  # 标准化连续型变量
data[c("per_cost","travel_days","photo_num","title_length")]=
 X_num

n=dim(data)[1]
q=dim(data)[2]-1
id = sample(x=1:n,size=round(n*0.2))  # 划分测试集的索引
data_train=data[-id,]
data_test = data[id,]

ano_frame=anova(lm(read_num_transformed~.,data=data))
rownames(ano_frame)[ano_frame['Pr(>F)']<0.001&!is.na(ano_frame['Pr(>F)'])]

X1 = data[c("travel_days","photo_num","person_num_亲子","title_length","playmode1","playmode2" )]
result = cbind(X1,y)
```

## Lasso, Ridge regression, Elastic net

为了解决线性回归出现的过拟合以及在通过正规方程方法求解θ的过程中出现的$X^\mathrm{T}X$不可逆，可在损失函数中引入正则化项来达到目的。为了防止过拟合(θ过大)，在目标函数$J(\theta)$后添加复杂度惩罚因子，即正则项来防止过拟合。正则项可以使用$L_1$-norm(Lasso)、$L_2$-norm(Ridge)，或结合$L_1$-norm、$L_2$-norm(Elastic Net)。

### Lasso
本节使用Lasso进行回归建模。回归复杂度调整的程度由参数$\lambda$来控制,$\lambda$越大对变量较多的线性模型的惩罚力度就越大，从而最终获得一个变量较少的模型。另一个参数$\alpha$控制应对高相关性数据时模型的性状。LASSO 回归时，$\alpha=1$。在实验中，先采取交叉验证在训练集中选出最优参数$\lambda$，建立模型，计算测试集的均方误差。
 
```{r}
library(glmnet)
#默认十折交叉验证选取最优参数
q=6
alpha1.fit <- cv.glmnet(as.matrix(data_train[,-(q+1)]), data_train[,q+1], type.measure="mse", alpha=1, family="gaussian")
plot(alpha1.fit)
```
```{r}
bestlam = alpha1.fit$lambda.min
bestlam
```

```{r}
#代入最优参数建模
bestlasso = glmnet(as.matrix(data_train[,-(q+1)]), data_train[,q+1], alpha = 1, lambda = bestlam)
bestlasso.pred = predict(bestlasso,as.matrix(data_test[,-(q+1)]))
#求出MSE
lasso_mse=mean((bestlasso.pred-data_test$read_num_transformed)^2)
cat('The MSE of Lasso is',lasso_mse)
```
```{r}
# 计算绝对误差小于0.5的预测比率 
lasso_diff = abs(data_test[,'read_num_transformed']-bestlasso.pred)
lasso_ratio = sum(lasso_diff<0.5)/length(lasso_diff)
```



### Ridge regression
本节使用Ridge regression进行回归建模。与Lasso处理方式类似，先求出最优参数$\lambda$，再建模拟合，此时$\alpha=0$。

```{r}
alpha2.fit <- cv.glmnet(as.matrix(data_train[,-(q+1)]), data_train[,q+1], type.measure="mse", alpha=0, family="gaussian")
plot(alpha2.fit)
```
```{r}
bestlam = alpha2.fit$lambda.min
bestlam
```
```{r}
#代入最优参数建模
bestridge = glmnet(as.matrix(data_train[,-(q+1)]), data_train[,q+1], alpha = 0, lambda = bestlam)
bestridge.pred = predict(bestridge,as.matrix(data_test[,-(q+1)]))
#求出MSE
ridge_mse=mean((bestridge.pred-data_test$read_num_transformed)^2)
cat('The MSE of Ridge regression is',ridge_mse)
```
```{r}
# 计算绝对误差小于0.5的预测比率 
ridge_diff = abs(data_test[,'read_num_transformed']-bestridge.pred)
ridge_ratio = sum(ridge_diff <0.5)/length(ridge_diff )
```


### Elastic net
本节使用Elastic net进行回归建模。

```{r}
list.of.fits <- list()
for (i in 0:20) {
  fit.name <- paste0("alpha", i/20)
  list.of.fits[[fit.name]] <-
    cv.glmnet(as.matrix(data_train[,-(q+1)]), data_train[,q+1], type.measure="mse", alpha=i/20, 
      family="gaussian")
}
results <- data.frame()
for (i in 0:20) {
  fit.name <- paste0("alpha", i/20)
  predicted <- 
    predict(list.of.fits[[fit.name]], 
      s=list.of.fits[[fit.name]]$lambda.min, newx=as.matrix(data_test[,-(q+1)]))
  
  #计算MSE
  mse <- mean((data_test$read_num_transformed - predicted)^2)
  #存储结果
  temp <- data.frame(alpha=i/20, mse=mse, fit.name=fit.name)
  results <- rbind(results, temp)
}

results
```
```{r}
plot(results$alpha,results$mse,xlab = "Alpha",ylab = "MSE")
```
```{r}
results[which.min(results$mse),]
```
```{r}
bestElastic = glmnet(as.matrix(data_train[,-(q+1)]), data_train[,q+1], alpha = 0.2, lambda =list.of.fits$alpha0.2$lambda.min )
bestElastic.pred = predict( bestElastic,as.matrix(data_test[,-(q+1)]))
#求出MSE
Elastic_mse=mean((bestElastic.pred-data_test$read_num_transformed)^2)
cat('The MSE of Elastic Net is',Elastic_mse)
```
```{r}
# 计算绝对误差小于0.5的预测比率 
elastic_diff = abs(data_test[,'read_num_transformed']-bestElastic.pred)
elastic_ratio = sum(elastic_diff<0.5)/length(elastic_diff)
```

## PCR
本节使用PCR进行回归建模。使用80\%的数据作为训练数据建立主成分回归模型，再使用20\%的数据作为测试数据进行检验。
```{r}
pr<-princomp(data,cor=T)
screeplot(pr,type="lines",main = "PCR")
```
```{r}
pre<-predict(pr)
data.pr=data
data.pr$z1<-pre[,1]
data.pr$z2<-pre[,2]
data.pr$z3<-pre[,3]
data.pr$z4<-pre[,4]
data.pr$z5<-pre[,5]
data.pr$z6<-pre[,6]
lm.pcr<-lm(data.pr$read_num_transformed~z1+z2+z3+z4+z5+z6,data = data.pr)
summary(lm.pcr)
```

```{r}
beta=coef(lm.pcr)
A<-loadings(pr)
data_test.pr=matrix(1,nrow=dim(data_test)[1],ncol=dim(data_test)[2])
data_test.pr[,-1]=as.matrix(data_test[,-(q+1)])
pr.pred=apply(data_test.pr%*%as.matrix(A[,1:6]),1,sum)
pcr_mse=mean((pr.pred-data_test$read_num_transformed)^2)
cat('The MSE of PCR is',pcr_mse)
```


## PLS
本节使用PLS进行回归建模。
```{r}
library(pls)
pls1 <- plsr(data$read_num_transformed~., data = data, validation = "CV")
summary(pls1,what="all")
```




## KNN
本节使用KNN进行回归建模。由于K近邻法适用于特征空间维数度较低的数据，当特征空间维度增加时较小的近邻比率就需要较大的期望距离，K近邻法的局部性就会逐渐丧失，导致预测偏差增大，因而我们需要先进行特征选择。这里采用方差分析法选取与响应变量有显著线性相关性的输入变量。
```{r}
# 利用方差分析进行特征选择
ano_frame=anova(lm(read_num_transformed~.,data=data))
rownames(ano_frame)[ano_frame['Pr(>F)']<0.001&!is.na(ano_frame['Pr(>F)'])]
```
可见变量“content_count”，“photo_num”，“playmode_探险”，“playmode_赏秋”，“playmode_短途周末”，“playmode_美食”是显著的，因而我们选取这些特征作为输入变量。设定KNN的距离度量为欧式距离，接下来利用交叉验证搜索最佳的K值。
```{r}
X1 = data[c("travel_days","photo_num","person_num_亲子","title_length","playmode1","playmode2" )]
data1 = cbind(X1,y)[-id,]  # 训练集
n = dim(data1)[1]
col = dim(data1)[2]
library('class')
set.seed(12345)
sample_id = sample(x=1:n)
mse_vector = vector()  # 均方误差向量
# 利用交叉验证搜索最优的K
for (i in 1:100){
n1 = round(n/5)
for (j in 1:5){  # 5折交叉验证
mse = vector() 
data_test = data1[sample_id[(n1*(j-1)+1):min(n1*j,n)],]
data_train = data1[-sample_id[(n1*(j-1)+1):min(n1*j,n)],]
knn_fit = knn(train=data_train[,-col],test=data_test[,-col],cl=data_train[,col],
              k=i,prob=FALSE)
knn_fit = as.double(as.vector(knn_fit))  # 回归结果为因子向量，需转换成数值型向量
mse = c(mse,sum((data_test[,col]-knn_fit)^2)/length(data_test[,col]))
}
mse_vector = c(mse_vector, mean(mse))
}
plot(mse_vector,type='l',xlab='K',ylab='MSE',main='MSE of KNN model and K')
k_best = which(mse_vector==min(mse_vector))
k_best
```
可见当K取2时，KNN模型在测试集上的均方误差最小，故选取K=2，得到我们的KNN模型。以下选取80\%的样本作训练集，20\%的样本作测试集给出K=2时KNN模型的均方误差。
```{r}
# KNN, K=2
train = cbind(X1,y)[-id,]
test = cbind(X1,y)[id,]
knn_model=knn(train=train,test=test,cl=train[,col],
              k=2,prob=FALSE)
knn_model_predict=as.double(as.vector(knn_model))
knn_mse=sum((test[,col]-knn_model_predict)^2)/length(test[,col])
cat('The MSE of KNN is',knn_mse)

# 计算绝对误差小于0.5的预测比率 
knn_diff = abs((test[,col]-knn_model_predict))
knn_ratio = sum(knn_diff<0.5)/length(knn_diff) 
```

## SVM
本节使用SVM进行回归建模。我们将数据集80\%的样本作训练集，20\%的样本作测试集，其划分和使用KNN方法时一致，以便于比较两者的预测表现。而后使用tune.svm函数，在训练集上自动实现10折交叉验证选择最优惩罚参数C，得到最优模型后在测试集上进行预测，并给出SVM模型的均方误差MSE。
```{r}
# SVM回归
data_train=data[-id,]
data_test=data[id,]
library('e1071')
C=c(0.001,0.01,0.1,1,5,10,100,1000)  # 候选惩罚参数向量
svm_fit=tune.svm(read_num_transformed~.,data=data_train,type='eps-regression',cost=C,na.action=na.omit)  # 核函数默认为径向基核
summary(svm_fit)
best_svm = svm_fit$best.model  # 最优回归SVM模型
summary(best_svm)
y_pred = predict(best_svm,data_test)
svm_mse = mean((data_test[,'read_num_transformed']-y_pred)^2)
cat('The MSE of SVM is',svm_mse)

# 计算绝对误差小于0.5的预测比率 
svm_diff = abs(data_test[,'read_num_transformed']-y_pred)
svm_ratio = sum(svm_diff<0.5)/length(svm_diff)
```

## Decision tree
本节使用决策树进行回归建模。决策树的预测是基于逻辑的，避免了传统统计中一般线性模型、广义线性模型、判别分析等对数据分布的要求，能够在无分布限制的宽松环境下找出数据中输入变量与响应变量取值间的逻辑对应关系或规则，并实现对新数据响应变量的预测。其建模分为两步：决策树的生成和决策树的剪枝。

我们使用rpart.control函数执行自动设置预修剪等的参数，其采用的是N折交叉验证法。而后使用rpart函数进行决策树回归，并在20\%测试集上预测，计算MSE。
```{r}
# 决策树回归
data_train=data[-id,]
data_test=data[id,]
library('rpart')
library('rpart.plot')
ctl = rpart.control(minsplit=50,xval=5,maxdepth=10,cp=0)  
# 自行设定修剪参数，复杂度参数CP初值为0，采用5折交叉验证
set.seed(12345)
tree_fit=rpart(read_num_transformed~.,data=data_train,
               method='anova',control=ctl)  # 建立回归树
# printcp(tree_fit)
tree_pred=predict(tree_fit,data_test)  # 预测
tree_mse = mean((data_test[,'read_num_transformed']-tree_pred)^2)
cat('The MSE of Decision tree is',tree_mse)

# 计算绝对误差小于0.5的预测比率 
tree_diff = abs(data_test[,'read_num_transformed']-tree_pred)
tree_ratio = sum(tree_diff<0.5)/length(tree_diff)
```

## Random forest
本节使用随机森林进行回归建模。随机森林是将基础学习器决策树组合起来的强模型，其特色在于随机。其训练样本是对原始样本的重抽样，和交叉验证的思想一致，训练样本具有随机性；在每棵决策树的建立过程中，成为当前最佳分组变量的输入变量是输入变量全体的一个随机候选变量子集中的最优者，分组变量也具有随机性。建立在不同随机训练样本集上的决策树基本学习器能够体现数据的各个局部的关系，这样的组合预测模型能达到比简单的决策树更理想的效果。

以下采用randomForest函数进行数据回归，其重抽样过程已达到交叉验证的效果，而后在20\%测试集上预测，并计算MSE。
```{r,fig.width=12,fig.height=6}
# 随机森林回归
data_train=data[-id,]
data_test=data[id,]
library('randomForest')
set.seed(12345)
rf_fit=randomForest(read_num_transformed~.,data=data_train,
                    importance=TRUE)
plot(rf_fit,main='error and the number of trees')
rf_pred = predict(rf_fit,data_test)
rf_mse = mean((data_test[,'read_num_transformed']-rf_pred)^2)
cat('The MSE of Random forest is',rf_mse)
# importance(rf_fit,type=1)
varImpPlot(rf_fit,sort=TRUE,n.var=10)  # 评价输入变量重要性

# 计算绝对误差小于0.5的预测比率 
rf_diff = abs(data_test[,'read_num_transformed']-rf_pred)
rf_ratio = sum(rf_diff<0.5)/length(rf_diff)
```
此外，根据输入变量重要性图，从对相应变量预测精度(MSE)影响的角度来看，输入变量“photo_num”，“content_count”，“travel_days”，“playmode_短途周末”,“per_cost”较为重要。

## XgBoost
本节使用XgBoost进行回归建模。XgBoost基本思想和梯度提升决策树(GBDT)相同，但做了很多优化。XgBoost利用泰勒展开和二阶导数使损失函数更精确；利用正则化技术简化模型，避免过拟合；采取Block存储可以并行计算等。这些使得XgBoost更为高效和灵活。

以下在80\%样本的训练集上采用交叉验证确定XgBoost的最佳迭代次数，得到XgBoost模型，而后在20\%测试集上预测，并计算MSE。
```{r}
# XgBoost回归
data_train=data[-id,]
data_test=data[id,]
library('xgboost')
library('Matrix')
set.seed(12345)
# 处理训练集数据格式
train_x = Matrix(data.matrix(data_train[,-dim(data_train)[2]])
                 ,sparse=T)  # 将自变量转化为稀疏矩阵
train_y = data.matrix(data_train[,'read_num_transformed'])
# 将响应变量转化为numeric
train_xgb = list(data=train_x,label=train_y)  # 拼接为list
d_train = xgb.DMatrix(data=train_xgb$data,
                      label=train_xgb$label)
# 构建模型需要的xgb.DMatrix对象

# 处理测试集数据格式
test_x = Matrix(data.matrix(data_test[,-dim(data_test)[2]])
                 ,sparse=T)  # 将自变量转化为稀疏矩阵
d_test = xgb.DMatrix(data=test_x) 
# 构建模型需要的xgb.DMatrix对象

# XgBoost建模
param = list(max_depth=10,eta=0.3) # 定义模型参数 
cv=xgb.cv(params=param,data=d_train,nrounds = 20,
                 nfold = 5,verbose=FALSE)  # 5折交叉验证 
cv_test_mse = cv$evaluation_log[,'test_rmse_mean']
nrounds=which(cv_test_mse==min(cv_test_mse))

xgb_fit = xgboost(params=param,data=d_train,nrounds=nrounds)
xgb_pred = predict(xgb_fit,d_test)
xgb_mse=mean((data_test[,'read_num_transformed']-xgb_pred)^2)
cat('The MSE of XgBoost is',xgb_mse)

# 计算绝对误差小于0.5的预测比率 
xgb_diff = abs(data_test[,'read_num_transformed']-xgb_pred)
xgb_ratio = sum(xgb_diff<0.5)/length(xgb_diff)

```


## K-Means
本节使用K-Means对数据进行聚类分析。进行K-Means聚类需要确定聚类数目K和初始类质心。我们依据最大的调整的类间离差平方和与类内离差平方总和的比$\frac{betweenss}{K-1}/\frac{tot.withinss}{n-K}$来选择合理的K，而初始类质心则通过kmeans函数的nstart参数来多次重复随机抽取，并选取使聚类后tot.withinss最小的初始类质心。
```{r}
# K-Means 聚类
set.seed(12345)
n=dim(data)[1]
ratio_km=vector()
for(K in 2:20){
km_cluster = kmeans(x=data,centers=K,nstart=30)
ratio_km = c(ratio_km,km_cluster$betweenss/(K-1)/
  (km_cluster$tot.withinss/(n-K)))
}
K=which.max(ratio_km)  # 选取最优的K
km_cluster = kmeans(x=data,centers=K,nstart=30)
cat('The best K is', K,'\n')
cat('The class centers are:\n')
km_cluster$centers[,1:6]
```
可以看到，在$2\leq K\leq20$范围内，最优的K=2，我们据此建立了聚类模型，并输出了各类的类质心。

## EM
本节采用EM算法进行聚类分析。EM聚类是基于统计分布的聚类模型，其与K-Means聚类的区别在于类别指派的依据是概率，其想法是极大似然估计。我们使用Mclust包中的mclust实现EM聚类，结果如下。
```{r}
library(mclust)
em_cluster = Mclust(data=data)
summary(em_cluster)
```

## Neural network
本节使用前馈神经网络FNN进行回归建模，采用B-P反向传播算法，即通过梯度下降法在每轮迭代中调整权重。在FNN中设定1层隐藏层和2个隐节点，损失函数采用平方误差，激活函数采用Sigmoid函数。而后用80\%样本的测试集训练FNN，并计算在20\%测试集上的MSE。
```{r}
# FNN 回归建模
# 采用经过方差分析特征选择后的数据
X1 = X[c('content_count','photo_num','playmode_赏秋',
         'playmode_探险','playmode_短途周末','playmode_美食')]
train = cbind(X1,y)[-id,]
test = cbind(X1,y)[id,]
library(neuralnet)
set.seed(12345)
fnn_fit=neuralnet(read_num_transformed~.,data=train,
                  hidden=2,err.fct='sse',linear.output=FALSE)
fnn_pred=predict(fnn_fit,test)
fnn_mse=mean((test[,'read_num_transformed']-fnn_pred)^2)
cat('The MSE of FNN is',fnn_mse)

# 计算绝对误差小于0.5的预测比率 
fnn_diff = abs(data_test[,'read_num_transformed']-fnn_pred)
fnn_ratio = sum(fnn_diff<0.5)/length(fnn_diff)

```

## Compare different models
本节比较KNN,SVM,Decision tree,Random forest,XgBoost,Neural network在测试集上的表现，绘制它们的MSE箱线图和预测绝对误差小于0.5的占比条形图。
```{r,fig.width=12,fig.height=6}
models = c('Lasso','Ridge regression','Elastic Net', 'KNN', 'SVM', 'Decision tree', 'Random forest', 'XgBoost', 'Neural network')
models = as.factor(models)
mse_all = cbind(lasso_diff,ridge_diff,elastic_diff,knn_diff,svm_diff,tree_diff,rf_diff,xgb_diff,fnn_diff)
mse_frame = data.frame(data=mse_all)
mse_frame=`colnames<-`(mse_frame,models)  # 重命名列名
ratio_all=cbind(lasso_ratio,ridge_ratio,elastic_ratio, knn_ratio,svm_ratio,tree_ratio,rf_ratio,xgb_ratio,fnn_ratio)
par(mfrow=c(1,2),cex=0.7,cex.main=1,font.main=1)
# palette = RColorBrewer::brewer.pal(6,'Set1')  
# 设置离散型调色板
boxplot(mse_frame,main='MSE on test data',
     xlab='models',ylab='MSE')
barplot(ratio_all,names=models,density=40,
        main='ratio barplot of absolute error less than 
        0.5 on test data',xlab='models',ylab='ratio')
```

可以看到，KNN算法在预测阅读数响应变量上的表现最好。